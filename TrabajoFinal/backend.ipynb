{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backend Server"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Procesamiento de la imagen en el servidor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "\n",
    "model = YOLO(model='sign_language_1.pt')\n",
    "classNames = ['A', 'B', 'C', 'D', 'E', 'F', 'G',\n",
    "              'H', 'I', 'J', 'K', 'L', 'M', 'N', \n",
    "              'O', 'P', 'Q', 'R', 'S', 'T', 'U',\n",
    "              'V', 'W', 'X', 'Y', 'Z']\n",
    "\n",
    "def isLetterInImage(frame, target_letter, confidence_threshold=0.3):\n",
    "    # Realiza la detección utilizando el modelo\n",
    "    results = model(frame, stream=True)\n",
    "    \n",
    "    # Para cada detección\n",
    "    for r in results:\n",
    "        boxes = r.boxes\n",
    "        for box in boxes:\n",
    "            x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
    "            confidence = box.conf[0]\n",
    "\n",
    "            # Filtra las detecciones con una confianza superior al umbral\n",
    "            if confidence >= confidence_threshold:\n",
    "                cls = int(box.cls[0])\n",
    "                \n",
    "                # Convierte el identificador numérico de la clase a un nombre de clase\n",
    "                class_name = classNames[cls] if cls < len(classNames) else 'Unknown'\n",
    "                \n",
    "                # Verifica si la detección coincide con la letra objetivo\n",
    "                if class_name == target_letter:\n",
    "                    return True  # Letra objetivo detectada\n",
    "\n",
    "    return False  # Letra objetivo no detectada\n",
    "\n",
    "# Ejemplo de uso\n",
    "image_path = './assets/test_image_A.jpg'  # Reemplaza con la ruta de tu imagen\n",
    "target_letter = 'L'  # Reemplaza con la letra objetivo\n",
    "\n",
    "frame = cv2.imread(image_path)\n",
    "\n",
    "# result = isLetterInImage(frame, target_letter)\n",
    "# print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Procesamiento de video en el servidor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "import numpy as np\n",
    "\n",
    "def isLetterInVideo(video_path, target_letter, model_path='sign_language_1.pt', confidence_threshold=0.3, percentage_threshold=60):\n",
    "    model = YOLO(model=model_path)\n",
    "    classNames = ['A', 'B', 'C', 'D', 'E', 'F', 'G',\n",
    "                  'H', 'I', 'J', 'K', 'L', 'M', 'N', \n",
    "                  'O', 'P', 'Q', 'R', 'S', 'T', 'U',\n",
    "                  'V', 'W', 'X', 'Y', 'Z']\n",
    "\n",
    "    # Lista de colores en formato hexadecimal\n",
    "    colores_hex = [\"#09A9F6\", \"#0045FF\", \"#4200FF\", \"#A500FF\", \"#D600FF\", \"#FB048F\", \"#FB3504\", \n",
    "                   \"#FF7800\", \"#F9BA06\", \"#9CDF05\", \"#33B702\", \"#07900A\", \"#03C460\", \"#03BAD5\",\n",
    "                   \"#C7570C\", \"#AEC200\", \"#414344\", \"#B800D0\", \"#0038FF\", \"#A700AA\", \"#7D8182\",\n",
    "                   \"#0B80F4\", \"#C809F6\", \"#C3B500\", \"#09A9F6\", \"#5E1EFF\", ]\n",
    "\n",
    "    # Convierte los colores hexadecimales a RGB\n",
    "    colores_rgb = [(int(col[1:3], 16), int(col[3:5], 16), int(col[5:7], 16)) for col in colores_hex]\n",
    "\n",
    "    # Crea un diccionario que asigna cada letra a un color en formato RGB\n",
    "    letras_colores = {letra: color for letra, color in zip(classNames, colores_rgb)}\n",
    "\n",
    "\n",
    "    \n",
    "    # Abre el archivo de video\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    \n",
    "    # Crea el video de salida con el mismo nombre pero con \"output_\" como prefijo\n",
    "    output_video_path = video_path.replace(video_path.split('/')[-1], f\"output_{video_path.split('/')[-1]}\")\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'H264')\n",
    "    out = cv2.VideoWriter(output_video_path, fourcc, 30, (int(cap.get(3)), int(cap.get(4))))\n",
    "    \n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    detected_frames = 0\n",
    "\n",
    "    \n",
    "    while True:\n",
    "        ret, img = cap.read()\n",
    "        \n",
    "        if ret:\n",
    "            # Realiza la detección utilizando el modelo\n",
    "            results = model(img, stream=True)\n",
    "\n",
    "            # Para cada detección\n",
    "            for r in results:\n",
    "                boxes = r.boxes\n",
    "                for box in boxes:\n",
    "                    x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
    "                    confidence = box.conf[0]\n",
    "\n",
    "                    # Filtra las detecciones con una confianza superior al umbral\n",
    "                    if confidence >= confidence_threshold:\n",
    "                        cls = int(box.cls[0])\n",
    "\n",
    "                        # Convierte el identificador numérico de la clase a un nombre de clase\n",
    "                        class_name = classNames[cls] if cls < len(classNames) else 'Unknown'\n",
    "\n",
    "                        # Verifica si la clase detectada es la letra objetivo\n",
    "                        if class_name == target_letter:\n",
    "                            detected_frames += 1\n",
    "                            \n",
    "                        # Dibuja el rectángulo de fondo del texto\n",
    "                        cv2.rectangle(img, (x1, y1 - 35), (x2, y1), color=letras_colores[class_name], thickness=-1)  # -1 para rellenar el rectángulo\n",
    "                        cv2.rectangle(img, (x1, y1), (x2, y2), color=letras_colores[class_name] , thickness=2)\n",
    "                        # Dibuja el texto en el recuadro de fondo\n",
    "                        cv2.putText(img, f\"{class_name} {confidence:.2f}\", (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255, 255, 255), thickness=2)\n",
    "\n",
    "            # Escribe el frame procesado en el video de salida\n",
    "            out.write(img)\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    # Libera la cámara y cierra las ventanas abiertas\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "        \n",
    "    # Guarda el video de salida\n",
    "    out.release()\n",
    "    # Calcula el porcentaje de detección de la letra objetivo\n",
    "    detection_percentage = (detected_frames / total_frames) * 100\n",
    "\n",
    "    # Retorna True si el porcentaje es superior al umbral, False en caso contrario\n",
    "    return detection_percentage >= percentage_threshold\n",
    "\n",
    "# Ejemplo de uso de la función\n",
    "input_video_path = './assets/test_video_a.mp4'\n",
    "target_letter = 'A'\n",
    "\n",
    "# result = isLetterInVideo(input_video_path, target_letter)\n",
    "# print(f\"¿Se detectó la letra {target_letter} en más del 60% del video?: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Servidor con imagen y video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on http://10.22.146.19:5000\n",
      "Press CTRL+C to quit\n",
      "10.22.146.19 - - [16/Jan/2024 23:50:30] \"GET / HTTP/1.1\" 200 -\n",
      "10.22.146.19 - - [16/Jan/2024 23:50:30] \"GET /templates/style/images/AppLogo.png HTTP/1.1\" 304 -\n",
      "10.22.146.19 - - [16/Jan/2024 23:50:30] \"GET /templates/style/images/interactive-lessons.png HTTP/1.1\" 304 -\n",
      "10.22.146.19 - - [16/Jan/2024 23:50:30] \"GET /templates/style/images/real-time-detection.jpg HTTP/1.1\" 304 -\n",
      "10.22.146.19 - - [16/Jan/2024 23:50:30] \"GET /templates/style/css/styles.css HTTP/1.1\" 200 -\n",
      "10.22.146.19 - - [16/Jan/2024 23:50:30] \"GET /templates/style/images/community.png HTTP/1.1\" 304 -\n",
      "10.22.146.19 - - [16/Jan/2024 23:50:30] \"GET /templates/style/images/background.jpg HTTP/1.1\" 304 -\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask, jsonify, request, render_template\n",
    "import cv2, os\n",
    "import numpy as np\n",
    "from IPython.display import Image, display\n",
    "from werkzeug.utils import secure_filename\n",
    "import ssl\n",
    "\n",
    "\n",
    "app = Flask(__name__, static_folder='./templates')\n",
    "\n",
    "# Solo para HTTPS\n",
    "context = ssl.SSLContext(ssl.PROTOCOL_TLS_SERVER)\n",
    "context.load_cert_chain('./certificates/cert.pem', './certificates/key.pem')\n",
    "\n",
    "\n",
    "# Ruta para mostrar un HTML básico en la raíz del servidor\n",
    "@app.route('/')\n",
    "def home():\n",
    "    # Página principal\n",
    "    return render_template('homepage.html')\n",
    "\n",
    "# Ruta para procesar texto\n",
    "@app.route('/test-server', methods=['POST'])\n",
    "def process_text():\n",
    "    data = request.json  # Obtener datos JSON de la solicitud POST\n",
    "    if data and 'text' in data:\n",
    "        text = data['text']  # Obtener la cadena de texto del campo 'text'\n",
    "        # Puedes realizar cualquier procesamiento deseado en la cadena de texto aquí.\n",
    "        # Por ejemplo, puedes transformarla a mayúsculas.\n",
    "        text_uppercase = text.upper()\n",
    "        \n",
    "        # Devolver la cadena de texto procesada en formato JSON.\n",
    "        return jsonify({\"processed_text\": text_uppercase})\n",
    "    else:\n",
    "        return jsonify({\"error\": \"No se ha proporcionado una cadena de texto válida\"}), 400\n",
    "\n",
    "# Ruta para procesar la imagen\n",
    "@app.route('/process-image', methods=['POST'])\n",
    "def process_image():\n",
    "    # Verifica si se ha enviado un archivo de imagen en la solicitud\n",
    "    if 'image' not in request.files:\n",
    "        return jsonify({\"error\": \"No se ha proporcionado una imagen válida\"}), 400\n",
    "\n",
    "    file = request.files['image']\n",
    "    \n",
    "   # Lee la imagen desde el archivo\n",
    "    image = cv2.imdecode(np.fromstring(file.read(), np.uint8), cv2.IMREAD_UNCHANGED)\n",
    "    \n",
    "    # Rotar la imagen 90 grados en sentido horario\n",
    "    image = cv2.rotate(image, cv2.ROTATE_90_COUNTERCLOCKWISE)\n",
    "    \n",
    "    # Muestra la imagen utilizando OpenCV\n",
    "    # cv2.imshow('Received Image', image)  \n",
    "    # Muestra la imagen como un output de la celda\n",
    "    display(Image(data=cv2.imencode('.jpg', cv2.resize(image, (300, 400)))[1]))   \n",
    "        \n",
    "    # Procesa la imagen utilizando tu script de OpenCV\n",
    "    target_letter = request.form.get('target_letter')  # Obtiene la letra objetivo de la solicitud\n",
    "    result = isLetterInImage(image, target_letter)  # Llama a la función que procesa la imagen\n",
    "    print(\"Detected letter\", target_letter, \"on image ?\", result)\n",
    "    \n",
    "    # Devuelve el resultado (True o False) como respuesta\n",
    "    return jsonify({\"result\": result})\n",
    "\n",
    "# Ruta para procesar el video\n",
    "@app.route('/process-video', methods=['POST'])\n",
    "def process_video():\n",
    "    # Verifica si se ha enviado un archivo de vídeo en la solicitud\n",
    "    if 'video' not in request.files:\n",
    "        return jsonify({\"error\": \"No se ha proporcionado un video válido\"}), 400\n",
    "\n",
    "    video_file = request.files['video']\n",
    "    \n",
    "    # Guarda el video de manera segura y temporal\n",
    "    filename = secure_filename(video_file.filename)\n",
    "    temp_video_path = os.path.join('./tmp', filename)\n",
    "    \n",
    "    video_file.save(temp_video_path)\n",
    "    # Después de guardar el archivo\n",
    "    print(f\"Archivo guardado con éxito: {temp_video_path}\")\n",
    "    \n",
    "    # Procesa la imagen utilizando tu script de OpenCV\n",
    "    target_letter = request.form.get('target_letter')  # Obtiene la letra objetivo de la solicitud\n",
    "    result = isLetterInVideo(temp_video_path, target_letter)\n",
    " \n",
    "    # Elimina el archivo de video después de procesar\n",
    "    # os.remove(temp_video_path)\n",
    "    print(\"Detected letter\", target_letter, \"on video ?\", result)\n",
    "    \n",
    "    # Devuelve alguna respuesta\n",
    "    return jsonify({\"result\": result})\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(debug=False, host='10.22.146.19', port=5000) # Antes localhost\n",
    "    # app.run(debug=False, host='10.22.146.19', port=5000, ssl_context=context) # Para HTTPS\n",
    "    # Wifi Principal: 10.22.146.19\n",
    "    # Wifi Movil: 192.168.194.99"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "VC_TF",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
